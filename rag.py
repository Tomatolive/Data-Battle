import os
import json
import re
import pickle
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import torch
from typing import List, Dict, Any, Union, Tuple
import ollama
import requests


#################################
#   Chargeur de donn√©es adapt√©  #
#################################
def load_text_documents(root_dir: str) -> List[Dict[str, Any]]:
    """
    Charge tous les fichiers texte de la structure organis√©e en un format adapt√© au RAG

    Args:
        root_dir: Chemin racine des fichiers texte extraits

    Returns:
        Liste de documents avec m√©tadonn√©es
    """
    documents = []

    for dirpath, dirnames, filenames in os.walk(root_dir):
        for filename in filenames:
            if not filename.endswith(".txt"):
                continue

            file_path = os.path.join(dirpath, filename)

            # Extraire les m√©tadonn√©es du chemin
            rel_path = os.path.relpath(file_path, root_dir)
            parts = rel_path.split(os.sep)

            if len(parts) < 2:
                continue

            # Structure: ann√©e_type_partie_langue/groupe.txt
            exam_info = parts[0].split("_")

            if len(parts) >= 2 and len(exam_info) >= 3:
                try:
                    # Lire le contenu du fichier
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read().strip()

                    # Extraire l'ann√©e (premi√®re partie avant underscore)
                    year = exam_info[0] if exam_info[0].isdigit() else None

                    # D√©terminer le type de contenu
                    content_type = None
                    if "answers" in parts[0]:
                        content_type = "answers"
                    elif "ex-rep" in parts[0]:
                        content_type = "example_solutions"
                    elif "instructions" in parts[0]:
                        content_type = "instructions"
                    else:
                        content_type = "questions"

                    # D√©terminer la partie
                    part = None
                    for part_id in ["pt1", "pt2"]:
                        if part_id in parts[0]:
                            part = part_id
                            break

                    # Extraire le num√©ro de groupe
                    group = None
                    if "group_" in parts[1]:
                        group = int(parts[1].replace("group_", "").replace(".txt", ""))

                    # Cr√©er le document avec m√©tadonn√©es
                    document = {
                        "content": content,
                        "path": rel_path,
                        "year": year,
                        "part": part,
                        "content_type": content_type,
                        "group": group,
                        "filename": filename,
                    }

                    documents.append(document)

                except Exception as e:
                    print(f"Erreur lors du traitement de {file_path}: {e}")

    print(f"Charg√© {len(documents)} documents au total.")
    return documents


#########################################################
#   Organisation des chunks avec m√©tadonn√©es enrichies  #
#########################################################
def create_structured_chunks(
    documents: List[Dict[str, Any]],
    max_chunk_size: int = 1000,
    min_chunk_size: int = 100,
) -> List[Dict[str, Any]]:
    """
    Cr√©e des chunks en conservant la structure et les m√©tadonn√©es des documents

    Args:
        documents: Liste des documents charg√©s
        max_chunk_size: Taille maximale d'un chunk en caract√®res
        min_chunk_size: Taille minimale d'un chunk en caract√®res

    Returns:
        Liste de chunks avec m√©tadonn√©es
    """
    chunks = []

    for doc in documents:
        content = doc["content"]

        # Pour les documents courts, les garder intacts
        if len(content) <= max_chunk_size:
            chunk = {
                "text": content,
                "year": doc["year"],
                "part": doc["part"],
                "content_type": doc["content_type"],
                "group": doc["group"],
                "path": doc["path"],
                "is_complete_document": True,
            }
            chunks.append(chunk)
            continue

        # Pour les documents plus longs, diviser en paragraphes
        paragraphs = re.split(r"\n\s*\n", content)

        current_chunk = ""
        for para in paragraphs:
            if len(current_chunk) + len(para) <= max_chunk_size:
                current_chunk += para + "\n\n"
            else:
                # Enregistrer le chunk actuel s'il est assez grand
                if len(current_chunk) >= min_chunk_size:
                    chunks.append(
                        {
                            "text": current_chunk.strip(),
                            "year": doc["year"],
                            "part": doc["part"],
                            "content_type": doc["content_type"],
                            "group": doc["group"],
                            "path": doc["path"],
                            "is_complete_document": False,
                        }
                    )

                # Commencer un nouveau chunk
                current_chunk = para + "\n\n"

        # Ajouter le dernier chunk s'il est assez grand
        if len(current_chunk) >= min_chunk_size:
            chunks.append(
                {
                    "text": current_chunk.strip(),
                    "year": doc["year"],
                    "part": doc["part"],
                    "content_type": doc["content_type"],
                    "group": doc["group"],
                    "path": doc["path"],
                    "is_complete_document": False,
                }
            )

    return chunks


#################################################################################
#   Enrichissement des m√©tadonn√©es avec des liens entre questions et r√©ponses   #
#################################################################################
def link_questions_with_answers(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Enrichit les chunks en cr√©ant des r√©f√©rences entre questions et r√©ponses

    Args:
        chunks: Liste des chunks cr√©√©s

    Returns:
        Chunks enrichis avec des r√©f√©rences crois√©es
    """
    # Cr√©er des dictionnaires pour acc√©der rapidement aux chunks par ann√©e, partie et groupe
    questions_dict = {}
    answers_dict = {}
    solutions_dict = {}

    for i, chunk in enumerate(chunks):
        key = f"{chunk['year']}_{chunk['part']}_{chunk['group']}"

        if chunk["content_type"] == "questions":
            questions_dict[key] = i
        elif chunk["content_type"] == "answers":
            answers_dict[key] = i
        elif chunk["content_type"] == "example_solutions":
            solutions_dict[key] = i

    # Enrichir les chunks avec des r√©f√©rences
    for i, chunk in enumerate(chunks):
        key = f"{chunk['year']}_{chunk['part']}_{chunk['group']}"

        # Pour les questions, ajouter les r√©f√©rences aux r√©ponses et solutions
        if chunk["content_type"] == "questions":
            if key in answers_dict:
                chunks[i]["answer_ref"] = answers_dict[key]
            if key in solutions_dict:
                chunks[i]["solution_ref"] = solutions_dict[key]

        # Pour les r√©ponses, ajouter les r√©f√©rences aux questions et solutions
        elif chunk["content_type"] == "answers":
            if key in questions_dict:
                chunks[i]["question_ref"] = questions_dict[key]
            if key in solutions_dict:
                chunks[i]["solution_ref"] = solutions_dict[key]

        # Pour les solutions, ajouter les r√©f√©rences aux questions et r√©ponses
        elif chunk["content_type"] == "example_solutions":
            if key in questions_dict:
                chunks[i]["question_ref"] = questions_dict[key]
            if key in answers_dict:
                chunks[i]["answer_ref"] = answers_dict[key]

    return chunks


#########################################################################
#   Cr√©ation d'une base de connaissances avec m√©tadonn√©es filtrables    #
#########################################################################
def create_advanced_vector_db(
    chunks: List[Dict[str, Any]], model_name: str = "all-MiniLM-L6-v2"
):
    """
    Cr√©e une base de connaissances vectorielle avanc√©e avec filtres par m√©tadonn√©es

    Args:
        chunks: Liste des chunks enrichis
        model_name: Nom du mod√®le d'embeddings

    Returns:
        Composants du syst√®me RAG
    """
    # Charger le mod√®le d'embeddings
    model = SentenceTransformer(model_name)

    # Extraire les textes pour l'embedding
    texts = [chunk["text"] for chunk in chunks]

    # G√©n√©rer les embeddings
    print("G√©n√©ration des embeddings...\n")
    embeddings = model.encode(texts, show_progress_bar=True, batch_size=32)
    # Construire l'index FAISS
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings).astype("float32"))

    # Cr√©er des indices pour les m√©tadonn√©es pour faciliter le filtrage
    metadata_indices = {"year": {}, "part": {}, "content_type": {}, "group": {}}

    for i, chunk in enumerate(chunks):
        # Indexer par ann√©e
        year = chunk["year"]
        if year not in metadata_indices["year"]:
            metadata_indices["year"][year] = []
        metadata_indices["year"][year].append(i)

        # Indexer par partie
        part = chunk["part"]
        if part not in metadata_indices["part"]:
            metadata_indices["part"][part] = []
        metadata_indices["part"][part].append(i)

        # Indexer par type de contenu
        content_type = chunk["content_type"]
        if content_type not in metadata_indices["content_type"]:
            metadata_indices["content_type"][content_type] = []
        metadata_indices["content_type"][content_type].append(i)

        # Indexer par groupe
        group = chunk["group"]
        if group not in metadata_indices["group"]:
            metadata_indices["group"][group] = []
        metadata_indices["group"][group].append(i)

    return {
        "index": index,
        "chunks": chunks,
        "model": model,
        "metadata_indices": metadata_indices,
    }


#################################################
#   Fonction de recherche avanc√©e avec filtres  #
#################################################
def filtered_semantic_search(
    query: str,
    vector_db: Dict[str, Any],
    filters: Dict[str, Any] = None,
    top_k: int = 5,
):
    """
    Effectue une recherche s√©mantique avec filtres sur les m√©tadonn√©es

    Args:
        query: Requ√™te de recherche
        vector_db: Base de connaissances vectorielle
        filters: Filtres sur les m√©tadonn√©es (ann√©e, partie, type, groupe)
        top_k: Nombre de r√©sultats √† retourner

    Returns:
        Liste des chunks les plus pertinents
    """
    print("üîç Encodage du texte pour la recherche :", query)
    # Cr√©er l'embedding de la requ√™te
    query_embedding = (
        vector_db["model"].encode([query])[0].reshape(1, -1).astype("float32")
    )
    print("üîç Recherche FAISS en cours...")
    distances, indices = vector_db["index"].search(query_embedding, top_k)
    print("üìä R√©sultats FAISS - Indices :", indices)
    print("üìä R√©sultats FAISS - Distances :", distances)
    print("‚úÖ Embedding g√©n√©r√© :", query_embedding.shape)
    # Si aucun filtre n'est sp√©cifi√©, effectuer une recherche globale
    if filters is None or not filters:
        distances, indices = vector_db["index"].search(query_embedding, top_k)
        results = []

        for i, idx in enumerate(indices[0]):
            if idx < 0 or idx >= len(vector_db["chunks"]):
                continue

            chunk = vector_db["chunks"][idx]
            results.append({"chunk": chunk, "score": float(distances[0][i])})

        return results

    # Si des filtres sont sp√©cifi√©s, rechercher dans un sous-ensemble
    candidate_indices = set()
    first_filter = True

    # Appliquer les filtres
    for filter_key, filter_value in filters.items():
        if (
            filter_key not in vector_db["metadata_indices"]
            or filter_value not in vector_db["metadata_indices"][filter_key]
        ):
            # Si le filtre ne correspond √† aucun document, renvoyer un r√©sultat vide
            return []

        # R√©cup√©rer les indices correspondant au filtre
        filtered_indices = set(vector_db["metadata_indices"][filter_key][filter_value])

        # Intersection avec les r√©sultats pr√©c√©dents ou initialisation
        if first_filter:
            candidate_indices = filtered_indices
            first_filter = False
        else:
            candidate_indices &= filtered_indices

    # Si aucun document ne correspond √† tous les filtres, renvoyer un r√©sultat vide
    if not candidate_indices:
        return []

    # Convertir en liste pour l'indexation
    candidate_list = list(candidate_indices)

    # Cr√©er un sous-index temporaire pour la recherche
    sub_embeddings = np.array(
        [vector_db["index"].reconstruct(i) for i in candidate_list]
    ).astype("float32")
    sub_index = faiss.IndexFlatL2(sub_embeddings.shape[1])
    sub_index.add(sub_embeddings)

    # Effectuer la recherche dans le sous-index
    distances, sub_indices = sub_index.search(
        query_embedding, min(top_k, len(candidate_list))
    )

    # R√©cup√©rer les r√©sultats
    results = []
    for i, sub_idx in enumerate(sub_indices[0]):
        if sub_idx < 0 or sub_idx >= len(candidate_list):
            continue

        idx = candidate_list[sub_idx]
        chunk = vector_db["chunks"][idx]

        results.append({"chunk": chunk, "score": float(distances[0][i])})

    return results


#####################################################################
#   G√©n√©ration de contexte enrichi pour des questions sp√©cifiques   #
#####################################################################
def generate_enriched_context(
    query: str,
    vector_db: Dict[str, Any],
    topic: str = None,
    year: str = None,
    part: str = None,
    content_types: List[str] = None,
    top_k: int = 5,
):
    """
    G√©n√®re un contexte enrichi pour une requ√™te sp√©cifique

    Args:
        query: Requ√™te de recherche
        vector_db: Base de connaissances vectorielle
        topic: Sujet sp√©cifique √† chercher
        year: Ann√©e sp√©cifique
        part: Partie sp√©cifique
        content_types: Types de contenu √† inclure
        top_k: Nombre de r√©sultats par cat√©gorie

    Returns:
        Contexte format√©
    """
    if content_types is None:
        content_types = ["questions", "answers", "example_solutions"]

    # Construire les filtres de base
    base_filters = {}
    if year is not None:
        base_filters["year"] = year
    if part is not None:
        base_filters["part"] = part

    # Enrichir la requ√™te avec le sujet si sp√©cifi√©
    enhanced_query = query
    if topic is not None:
        enhanced_query = f"{topic} {query}"

    all_results = []

    # Effectuer une recherche pour chaque type de contenu
    for content_type in content_types:
        # Ajouter le filtre de type de contenu
        filters = base_filters.copy()
        filters["content_type"] = content_type

        # Effectuer la recherche
        results = filtered_semantic_search(enhanced_query, vector_db, filters, top_k)
        all_results.extend(results)

    # Trier par score
    all_results.sort(key=lambda x: x["score"])

    # Construire le contexte enrichi
    context_parts = []

    # Fonction pour formater un chunk
    def format_chunk(chunk, prefix=""):
        source_info = f"[Ann√©e: {chunk['year']}, Partie: {chunk['part']}, "
        source_info += f"Groupe: {chunk['group']}, Type: {chunk['content_type']}]"

        return f"{prefix}{source_info}\n\n{chunk['text']}"

    # Ajouter les r√©sultats au contexte
    for result in all_results[:top_k]:
        chunk = result["chunk"]

        # Formater le chunk principal
        context_parts.append(format_chunk(chunk))

        # Ajouter les r√©f√©rences li√©es (questions, r√©ponses, solutions)
        if chunk["content_type"] == "questions" and "answer_ref" in chunk:
            answer_chunk = vector_db["chunks"][chunk["answer_ref"]]
            context_parts.append(
                format_chunk(answer_chunk, prefix="R√âPONSE ASSOCI√âE: ")
            )

            if "solution_ref" in chunk:
                solution_chunk = vector_db["chunks"][chunk["solution_ref"]]
                context_parts.append(
                    format_chunk(solution_chunk, prefix="SOLUTION ASSOCI√âE: ")
                )

        elif chunk["content_type"] == "answers" and "question_ref" in chunk:
            question_chunk = vector_db["chunks"][chunk["question_ref"]]
            context_parts.append(
                format_chunk(question_chunk, prefix="QUESTION ASSOCI√âE: ")
            )

    # Joindre les parties avec des s√©parateurs
    formatted_context = "\n\n" + "=" * 50 + "\n\n".join(context_parts)

    return formatted_context


########################################################
#   Script principal pour construire le syst√®me RAG    #
########################################################
def build_exam_rag_system(text_root_dir: str, output_dir: str):
    """
    Construit un syst√®me RAG complet adapt√© aux examens d'ing√©nieur brevet

    Args:
        text_root_dir: R√©pertoire racine des fichiers texte
        output_dir: R√©pertoire de sortie pour les composants RAG
    """
    print("√âtape 1: Chargement des documents texte...")
    documents = load_text_documents(text_root_dir)

    print("√âtape 2: Cr√©ation de chunks structur√©s...")
    chunks = create_structured_chunks(documents)

    print("√âtape 3: Enrichissement avec liens entre questions et r√©ponses...")
    enriched_chunks = link_questions_with_answers(chunks)

    print("√âtape 4: Cr√©ation de la base de connaissances vectorielle...")
    vector_db = create_advanced_vector_db(enriched_chunks)

    print("√âtape 5: Sauvegarde des composants...")
    os.makedirs(output_dir, exist_ok=True)

    # Sauvegarder les chunks
    with open(os.path.join(output_dir, "chunks.pkl"), "wb") as f:
        pickle.dump(enriched_chunks, f)

    # Sauvegarder l'index FAISS
    faiss.write_index(vector_db["index"], os.path.join(output_dir, "faiss_index.bin"))

    # Sauvegarder les m√©tadonn√©es
    with open(os.path.join(output_dir, "metadata_indices.json"), "w") as f:
        # Convertir les ensembles en listes pour la s√©rialisation JSON
        serializable_indices = {}
        for key, value in vector_db["metadata_indices"].items():
            serializable_indices[key] = {k: list(v) for k, v in value.items()}
        json.dump(serializable_indices, f)

    # Sauvegarder le mod√®le
    vector_db["model"].save(os.path.join(output_dir, "sentence_transformer_model"))

    print(f"Syst√®me RAG sauvegard√© dans {output_dir}")


def clean_generated_response(response, context):
    """Nettoie la r√©ponse g√©n√©r√©e pour supprimer les parties du contexte RAG avec des crit√®res moins stricts."""

    # Si la r√©ponse contient des marqueurs √©vidents du contexte RAG
    context_markers = [
        "[ann√©e:",
        "ann√©e:",
        "[source",
        "R√âPONSE ASSOCI√âE",
        "SOLUTION ASSOCI√âE",
        "=====",
    ]

    # V√©rifier si un des marqueurs est pr√©sent au d√©but de la r√©ponse
    for marker in context_markers:
        if marker.lower() in response.lower()[:200]:  # V√©rifier seulement au d√©but
            # Chercher les sections standard
            sections = [
                "QUESTION:",
                "SOLUTION:",
                "CRIT√àRES D'√âVALUATION:",
                "ERREURS COURANTES:",
            ]
            for section in sections:
                if section in response:
                    # Commencer √† partir de la premi√®re section trouv√©e
                    start_pos = response.find(section)
                    response = response[start_pos:].strip()
                    break
            break

    # Ne pas rejeter la r√©ponse bas√©e sur la similarit√©, simplement nettoyer les parties √©videntes du contexte
    # Si la r√©ponse ne contient aucune des sections attendues, essayer de l'am√©liorer
    if not any(section in response for section in ["QUESTION:", "SOLUTION:"]):
        # Chercher le premier paragraphe qui semble √™tre une question
        paragraphs = response.split("\n\n")
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph) > 50 and "?" in paragraph:
                # Reconstruire avec un format plus clair
                clean_response = "QUESTION:\n" + paragraph + "\n\n"
                # Ajouter les paragraphes suivants comme solution
                if i + 1 < len(paragraphs):
                    clean_response += "SOLUTION:\n" + "\n\n".join(paragraphs[i + 1 :])
                return clean_response

    return response


def generate_exam_question_with_ollama(topic, difficulty, context=""):
    """G√©n√®re une question d'examen en utilisant Ollama avec la bonne syntaxe API"""

    # Choisir un bon mod√®le
    model_name = "llama3"  # ou "mistral", "phi3", etc.

    # Cr√©er un prompt bien structur√©
    prompt = f"""
Cr√©e une question d'examen sur le sujet: {topic}
Niveau: {difficulty}

Format requis:

QUESTION:
[Ta question ici]

SOLUTION:
[Ta solution d√©taill√©e]

CRIT√àRES D'√âVALUATION:
[Les crit√®res]

ERREURS COURANTES:
[Les erreurs typiques]

Contexte additionnel pour t'aider:
{context}
"""

    try:
        # Appeler Ollama avec la syntaxe correcte
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                },
            },
        )

        if response.status_code == 200:
            try:
                result = response.json()
                generated_text = result.get("response", "")
            except json.JSONDecodeError:
                # Si nous avons toujours une erreur de d√©codage JSON, traiter comme stream
                text_response = response.text
                generated_text = ""

                # Traiter chaque ligne JSON s√©par√©ment
                for line in text_response.strip().split("\n"):
                    try:
                        line_data = json.loads(line)
                        if "response" in line_data:
                            generated_text += line_data["response"]
                    except json.JSONDecodeError:
                        continue
        else:
            generated_text = f"Erreur API: {response.status_code}"

        return {
            "question": generated_text,
            "topic": topic,
            "difficulty": difficulty,
            "source": "ollama",
        }

    except Exception as e:
        print(f"Erreur avec Ollama: {str(e)}")
        return {
            "question": f"Erreur: {str(e)}",
            "topic": topic,
            "difficulty": difficulty,
            "error": True,
        }


###############################################################################
#   Int√©gration avec le mod√®le g√©n√©ratif pour la g√©n√©ration de questions   #
###############################################################################
def generate_exam_question(
    topic, difficulty, vector_db, llm_components, year=None, part=None
):
    """G√©n√®re une question d'examen avec un exemple concret pour guider le mod√®le"""

    try:
        llm, tokenizer = llm_components

        # Prompt avec un exemple concret pour guider le mod√®le
        prompt = f"""
Tu dois cr√©er une question d'examen sur "{topic}" pour le concours d'ing√©nieur brevet europ√©en.
Niveau de difficult√©: {difficulty}

Voici un exemple du format attendu:

QUESTION:
En tenant compte de l'article 52 de la Convention sur le brevet europ√©en (CBE), expliquez quelles sont les trois conditions principales de brevetabilit√© d'une invention en droit europ√©en et donnez deux exemples d'exceptions √† la brevetabilit√© pr√©vues par l'article 53 CBE.

SOLUTION:
Les trois conditions principales de brevetabilit√© selon l'article 52 CBE sont:
1. Nouveaut√©: l'invention ne doit pas faire partie de l'√©tat de la technique.
2. Activit√© inventive: l'invention ne doit pas d√©couler de mani√®re √©vidente de l'√©tat de la technique pour un homme du m√©tier.
3. Application industrielle: l'invention doit pouvoir √™tre fabriqu√©e ou utilis√©e dans tout genre d'industrie.

Les exceptions √† la brevetabilit√© selon l'article 53 CBE comprennent:
- Les inventions contraires √† l'ordre public ou aux bonnes m≈ìurs
- Les vari√©t√©s v√©g√©tales ou races animales et les proc√©d√©s essentiellement biologiques d'obtention de v√©g√©taux ou d'animaux
- Les m√©thodes de traitement chirurgical ou th√©rapeutique du corps humain ou animal et les m√©thodes de diagnostic appliqu√©es au corps humain ou animal

CRIT√àRES D'√âVALUATION:
- Identification correcte des trois conditions de brevetabilit√©
- Explication pr√©cise de chaque condition
- Identification correcte d'au moins deux exceptions
- R√©f√©rence correcte aux articles pertinents de la CBE

ERREURS COURANTES:
- Confusion entre nouveaut√© et activit√© inventive
- Omission de l'application industrielle comme condition de brevetabilit√©
- Interpr√©tation incorrecte des exceptions
- Ne pas mentionner les bases l√©gales (articles de la CBE)

MAINTENANT, CR√âE UNE NOUVELLE QUESTION ORIGINALE SUR "{topic}" EN SUIVANT CE FORMAT.
"""

        # Configurer pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        inputs = tokenizer(
            prompt, return_tensors="pt", padding=True, truncation=True
        ).to(llm.device)

        # G√©n√©rer avec des param√®tres optimis√©s pour √©viter les r√©p√©titions
        with torch.no_grad():
            try:
                output = llm.generate(
                    input_ids=inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_new_tokens=800,  # Limiter pour √©viter les longues r√©p√©titions
                    min_new_tokens=100,  # Forcer un minimum de g√©n√©ration
                    temperature=0.8,
                    top_p=0.92,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    # Ces param√®tres sont cruciaux pour √©viter les r√©p√©titions
                    repetition_penalty=1.2,  # P√©naliser la r√©p√©tition
                    no_repeat_ngram_size=4,  # Interdire la r√©p√©tition de s√©quences de 4 tokens
                )

                response = tokenizer.decode(output[0], skip_special_tokens=True)

                # Extraire la question g√©n√©r√©e
                question_start = response.find("QUESTION:")
                if question_start != -1 and question_start > len(prompt) - 100:
                    generated_response = response[question_start:]
                else:
                    # Chercher apr√®s "MAINTENANT, CR√âE"
                    prompt_end = response.find("MAINTENANT, CR√âE")
                    if prompt_end != -1:
                        second_part = response[prompt_end:]
                        question_start = second_part.find("QUESTION:")
                        if question_start != -1:
                            generated_response = second_part[question_start:]
                        else:
                            # Chercher simplement apr√®s l'exemple
                            solution_end = response.find("ERREURS COURANTES:")
                            if solution_end != -1:
                                potential_response = response[
                                    solution_end + len("ERREURS COURANTES:") :
                                ]
                                # Chercher la premi√®re ligne qui ressemble √† une question
                                lines = potential_response.split("\n")
                                for i, line in enumerate(lines):
                                    if len(line) > 30 and "?" in line:
                                        generated_response = (
                                            "QUESTION:\n" + line + "\n\n"
                                        )
                                        # Ajouter les lignes suivantes comme solution
                                        generated_response += "SOLUTION:\n" + "\n".join(
                                            lines[i + 1 :]
                                        )
                                        break
                                else:
                                    generated_response = (
                                        "Impossible d'extraire une question valide."
                                    )
                            else:
                                generated_response = "Format de r√©ponse incorrect."
                    else:
                        generated_response = "Format de r√©ponse incorrect."

                # Nettoyer la r√©ponse g√©n√©r√©e
                if len(generated_response) > 50:
                    # Limiter le nombre de r√©p√©titions du sujet dans la question
                    topic_count = generated_response.lower().count(topic.lower())
                    if topic_count > 3:
                        paragraphs = generated_response.split("\n\n")
                        clean_paragraphs = []
                        seen_content = set()

                        for para in paragraphs:
                            # Ignorer les paragraphes tr√®s similaires √† ceux d√©j√† vus
                            para_simplified = "".join(para.lower().split())[:50]
                            if para_simplified not in seen_content:
                                clean_paragraphs.append(para)
                                seen_content.add(para_simplified)

                        generated_response = "\n\n".join(clean_paragraphs)

                # V√©rification finale du contenu
                if generated_response.count("QUESTION:") > 1:
                    # S'il y a plus d'une section "QUESTION:", garder seulement la premi√®re
                    first_q = generated_response.find("QUESTION:")
                    second_q = generated_response.find("QUESTION:", first_q + 1)
                    generated_response = generated_response[:second_q]

                return {
                    "question": generated_response,
                    "topic": topic,
                    "difficulty": difficulty,
                    "year_reference": year,
                    "part_reference": part,
                }

            except Exception as e:
                print(f"Erreur lors de la g√©n√©ration: {str(e)}")
                return {
                    "question": f"Erreur lors de la g√©n√©ration: {str(e)}",
                    "topic": topic,
                    "difficulty": difficulty,
                    "year_reference": year,
                    "part_reference": part,
                    "error": True,
                }

    except Exception as e:
        print(f"Exception g√©n√©rale: {str(e)}")
        return {
            "question": f"Une erreur s'est produite: {str(e)}",
            "topic": topic,
            "difficulty": difficulty,
            "year_reference": year,
            "part_reference": part,
            "error": True,
        }


#################################################
#   Int√©gration pour l'√©valuation de r√©ponses   #
#################################################
def evaluate_student_answer(
    question: str, student_answer: str, vector_db: Dict[str, Any], llm_components: tuple
):
    """
    √âvalue la r√©ponse d'un √©tudiant √† une question

    Args:
        question: Question pos√©e
        student_answer: R√©ponse de l'√©tudiant
        vector_db: Base de connaissances vectorielle
        llm_components: Mod√®le LLM et tokenizer

    Returns:
        √âvaluation de la r√©ponse
    """
    llm, tokenizer = llm_components

    # R√©cup√©rer le contexte pertinent
    context = generate_enriched_context(
        query=question[:100],  # Utiliser le d√©but de la question comme requ√™te
        vector_db=vector_db,
        content_types=["answers", "example_solutions"],
        top_k=3,
    )

    # Construire le prompt
    system_prompt = f"""
    Tu es un expert en √©valuation pour le concours d'ing√©nieur brevet europ√©en.
    Ta t√¢che est d'√©valuer la r√©ponse d'un √©tudiant √† la question suivante:
    
    Question:
    {question}
    
    R√©ponse de l'√©tudiant:
    {student_answer}
    
    Utilise les exemples et solutions ci-dessous pour √©valuer la r√©ponse:
    {context}
    
    Ton √©valuation doit inclure:
    1. Un score sur 10
    2. Une analyse d√©taill√©e des points forts et des points faibles
    3. Des suggestions d'am√©lioration sp√©cifiques
    4. Une comparaison avec les solutions de r√©f√©rence
    
    Format de sortie:
    SCORE: [/10]
    
    √âVALUATION D√âTAILL√âE:
    [Ton analyse]
    
    POINTS FORTS:
    [Liste des points forts]
    
    POINTS √Ä AM√âLIORER:
    [Liste des points √† am√©liorer]
    
    COMPARAISON AVEC LA R√âF√âRENCE:
    [Comparaison avec les r√©ponses mod√®les]
    
    CONSEILS:
    [Conseils pour am√©liorer]
    """

    # G√©n√©rer l'√©valuation
    inputs = tokenizer(system_prompt, return_tensors="pt").to(llm.device)

    with torch.no_grad():
        output = llm.generate(
            inputs.input_ids,
            max_new_tokens=1024,
            temperature=0.3,  # Temp√©rature plus basse pour l'√©valuation
            top_p=0.9,
            do_sample=True,
        )

    # D√©coder la sortie
    full_output = tokenizer.decode(output[0], skip_special_tokens=True)
    response = full_output[len(system_prompt) :].strip()

    # Extraire le score (si pr√©sent)
    score = None
    if "SCORE:" in response:
        score_line = re.search(r"SCORE:\s*(\d+(?:\.\d+)?)\s*/\s*10", response)
        if score_line:
            try:
                score = float(score_line.group(1))
            except:
                pass

    # Formater la r√©ponse
    formatted_response = {
        "evaluation": response,
        "score": score,
        "question": question,
        "student_answer": student_answer,
    }

    return formatted_response


if __name__ == "__main__":
    build_exam_rag_system("ocr_text", "test_output")
